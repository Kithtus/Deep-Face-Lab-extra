{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building blocks\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Downscale(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=5, *kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = nn.Conv2d(self.in_ch, self.out_ch, self.kernel_size,\n",
    "                               stride=2, padding=\"same\")\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "    def get_out_ch(self):\n",
    "        return self.out_ch\n",
    "\n",
    "class DownscaleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, ch, n_downscales, kernel_size):\n",
    "        super().__init__()\n",
    "        self.downs = []\n",
    "        last_ch = in_ch\n",
    "        for i in range(n_downscales):\n",
    "            cur_ch = ch*( min(2**i, 8)  )\n",
    "            self.downs.append ( Downscale(last_ch, cur_ch, kernel_size=kernel_size))\n",
    "            last_ch = self.downs[-1].get_out_ch()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def depth_to_space(x,size):\n",
    "    x = torch.permute(x,(0,2,3,1))\n",
    "    b,h,w,c = x.shape\n",
    "    oh, ow = h * size, w * size\n",
    "    oc = c // (size * size)\n",
    "    x = x.reshape((-1,h,w,size,size,oc,))\n",
    "    x = torch.permute(x,(0, 1, 3, 2, 4, 5))\n",
    "    x = x.reshape((-1, oc, oh, ow))\n",
    "    return x\n",
    "\n",
    "class Upscale(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3, *kwargs):\n",
    "        super().__init__(*kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch*4, kernel_size,\n",
    "                               padding=\"same\")\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = depth_to_space(x,2)\n",
    "        return x\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, ch, kernel_size=3, conv_dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d( ch, ch, kernel_size=kernel_size, padding='same', dtype=conv_dtype)\n",
    "        self.conv2 = nn.Conv2d( ch, ch, kernel_size=kernel_size, padding='same', dtype=conv_dtype)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = self.conv1(inp)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(inp + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_dims = 128\n",
    "e_dims = 64\n",
    "d_dims = 64\n",
    "d_mask_dims = 16\n",
    "input_ch = 3\n",
    "resolution = 96\n",
    "opts = \"ud\"\n",
    "use_fp16=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_ch, e_ch,opts=\"ud\", **kwargs ):\n",
    "        self.in_ch = in_ch\n",
    "        self.e_ch = e_ch\n",
    "        self.opts = opts\n",
    "        if 't' in self.opts:\n",
    "            self.down1 = Downscale(self.in_ch, self.e_ch, kernel_size=5)\n",
    "            self.res1 = ResidualBlock(self.e_ch)\n",
    "            self.down2 = Downscale(self.e_ch, self.e_ch*2, kernel_size=5)\n",
    "            self.down3 = Downscale(self.e_ch*2, self.e_ch*4, kernel_size=5)\n",
    "            self.down4 = Downscale(self.e_ch*4, self.e_ch*8, kernel_size=5)\n",
    "            self.down5 = Downscale(self.e_ch*8, self.e_ch*8, kernel_size=5)\n",
    "            self.res5 = ResidualBlock(self.e_ch*8)\n",
    "        else:\n",
    "            self.down1 = DownscaleBlock(self.in_ch, self.e_ch, n_downscales=4 if 't' not in self.opts else 5, kernel_size=5)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if 't' in self.opts:\n",
    "            x = self.down1(x)\n",
    "            x = self.res1(x)\n",
    "            x = self.down2(x)\n",
    "            x = self.down3(x)\n",
    "            x = self.down4(x)\n",
    "            x = self.down5(x)\n",
    "            x = self.res5(x)\n",
    "        else:\n",
    "            x = self.down1(x)\n",
    "        x = torch.flatten(x)\n",
    "        if 'u' in self.opts:\n",
    "            x = torch.norm(x,dim = -1)\n",
    "        return x\n",
    "\n",
    "    def get_out_res(self, res):\n",
    "        return res // ( (2**4) if 't' not in self.opts else (2**5) )\n",
    "\n",
    "    def get_out_ch(self):\n",
    "        return self.e_ch * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, d_ch, d_mask_ch, opts):\n",
    "        \"create Decoder as torch Module using previously defined building blocks\"\n",
    "        super().__init__()\n",
    "        self.opts = opts\n",
    "        conv_dtype = torch.float32\n",
    "\n",
    "        if 't' not in self.opts:\n",
    "            self.upscale0 = Upscale(in_ch, d_ch*8, kernel_size=3)\n",
    "            self.upscale1 = Upscale(d_ch*8, d_ch*4, kernel_size=3)\n",
    "            self.upscale2 = Upscale(d_ch*4, d_ch*2, kernel_size=3)\n",
    "            self.res0 = ResidualBlock(d_ch*8, kernel_size=3)\n",
    "            self.res1 = ResidualBlock(d_ch*4, kernel_size=3)\n",
    "            self.res2 = ResidualBlock(d_ch*2, kernel_size=3)\n",
    "\n",
    "            self.upscalem0 = Upscale(in_ch, d_mask_ch*8, kernel_size=3)\n",
    "            self.upscalem1 = Upscale(d_mask_ch*8, d_mask_ch*4, kernel_size=3)\n",
    "            self.upscalem2 = Upscale(d_mask_ch*4, d_mask_ch*2, kernel_size=3)\n",
    "\n",
    "            self.out_conv  = nn.Conv2d( d_ch*2, 3, kernel_size=1, padding='same', dtype=conv_dtype)\n",
    "\n",
    "            if 'd' in self.opts:\n",
    "                self.out_conv1 = nn.Conv2d( d_ch*2, 3, kernel_size=3, padding='same', dtype=conv_dtype)\n",
    "                self.out_conv2 = nn.Conv2d( d_ch*2, 3, kernel_size=3, padding='same', dtype=conv_dtype)\n",
    "                self.out_conv3 = nn.Conv2d( d_ch*2, 3, kernel_size=3, padding='same', dtype=conv_dtype)\n",
    "                self.upscalem3 = Upscale(d_mask_ch*2, d_mask_ch*1, kernel_size=3)\n",
    "                self.out_convm = nn.Conv2d( d_mask_ch*1, 1, kernel_size=1, padding='same', dtype=conv_dtype)\n",
    "            else:\n",
    "                self.out_convm = nn.Conv2d( d_mask_ch*2, 1, kernel_size=1, padding='same', dtype=conv_dtype)\n",
    "        else:\n",
    "            self.upscale0 = Upscale(in_ch, d_ch*8, kernel_size=3)\n",
    "            self.upscale1 = Upscale(d_ch*8, d_ch*8, kernel_size=3)\n",
    "            self.upscale2 = Upscale(d_ch*8, d_ch*4, kernel_size=3)\n",
    "            self.upscale3 = Upscale(d_ch*4, d_ch*2, kernel_size=3)\n",
    "            self.res0 = ResidualBlock(d_ch*8, kernel_size=3)\n",
    "            self.res1 = ResidualBlock(d_ch*8, kernel_size=3)\n",
    "            self.res2 = ResidualBlock(d_ch*4, kernel_size=3)\n",
    "            self.res3 = ResidualBlock(d_ch*2, kernel_size=3)\n",
    "\n",
    "            self.upscalem0 = Upscale(in_ch, d_mask_ch*8, kernel_size=3)\n",
    "            self.upscalem1 = Upscale(d_mask_ch*8, d_mask_ch*8, kernel_size=3)\n",
    "            self.upscalem2 = Upscale(d_mask_ch*8, d_mask_ch*4, kernel_size=3)\n",
    "            self.upscalem3 = Upscale(d_mask_ch*4, d_mask_ch*2, kernel_size=3)\n",
    "            self.out_conv  = nn.Conv2d( d_ch*2, 3, kernel_size=1, padding='same', dtype=conv_dtype)\n",
    "\n",
    "            if 'd' in self.opts:\n",
    "                self.out_conv1 = nn.Conv2d( d_ch*2, 3, kernel_size=3, padding='same', dtype=conv_dtype)\n",
    "                self.out_conv2 = nn.Conv2d( d_ch*2, 3, kernel_size=3, padding='same', dtype=conv_dtype)\n",
    "                self.out_conv3 = nn.Conv2d( d_ch*2, 3, kernel_size=3, padding='same', dtype=conv_dtype)\n",
    "                self.upscalem4 = Upscale(d_mask_ch*2, d_mask_ch*1, kernel_size=3)\n",
    "                self.out_convm = nn.Conv2d( d_mask_ch*1, 1, kernel_size=1, padding='same', dtype=conv_dtype)\n",
    "            else:\n",
    "                self.out_convm = nn.Conv2d( d_mask_ch*2, 1, kernel_size=1, padding='same', dtype=conv_dtype)\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.upscale0(z)\n",
    "        x = self.res0(x)\n",
    "        x = self.upscale1(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.upscale2(x)\n",
    "        x = self.res2(x)\n",
    "\n",
    "        if 't' in self.opts:\n",
    "            x = self.upscale3(x)\n",
    "            x = self.res3(x)\n",
    "\n",
    "        if 'd' in self.opts:\n",
    "            x = torch.sigmoid( depth_to_space(torch.concat( (self.out_conv(x),\n",
    "                                                                self.out_conv1(x),\n",
    "                                                                self.out_conv2(x),\n",
    "                                                                self.out_conv3(x)), -1), 2) )\n",
    "        else:\n",
    "            x = torch.sigmoid(self.out_conv(x))\n",
    "\n",
    "\n",
    "        m = self.upscalem0(z)\n",
    "        m = self.upscalem1(m)\n",
    "        m = self.upscalem2(m)\n",
    "\n",
    "        if 't' in self.opts:\n",
    "            m = self.upscalem3(m)\n",
    "            if 'd' in self.opts:\n",
    "                m = self.upscalem4(m)\n",
    "        else:\n",
    "            if 'd' in self.opts:\n",
    "                m = self.upscalem3(m)\n",
    "\n",
    "        m = torch.sigmoid(self.out_convm(m))\n",
    "\n",
    "        # if use_fp16:\n",
    "        #     x = tf.cast(x, tf.float32)\n",
    "        #     m = tf.cast(m, tf.float32)\n",
    "\n",
    "        return x, m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_dense_res = resolution // (32 if 'd' in opts else 16)\n",
    "\n",
    "class Inter(nn.Module):\n",
    "    def __init__(self, in_ch, ae_ch, ae_out_ch,opts = \"ud\", **kwargs):\n",
    "        self.in_ch, self.ae_ch, self.ae_out_ch = in_ch, ae_ch, ae_out_ch\n",
    "        self.opts = opts\n",
    "        in_ch, ae_ch, ae_out_ch = self.in_ch, self.ae_ch, self.ae_out_ch\n",
    "\n",
    "        self.dense1 = nn.Linear( in_ch, ae_ch )\n",
    "        self.dense2 = nn.Linear( ae_ch, lowest_dense_res * lowest_dense_res * ae_out_ch )\n",
    "        if 't' not in self.opts:\n",
    "            self.upscale1 = Upscale(ae_out_ch, ae_out_ch)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = inp\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = x.reshape((-1,lowest_dense_res, lowest_dense_res, self.ae_out_ch))\n",
    "        if 't' not in self.opts:\n",
    "            x = self.upscale1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_out_res(self):\n",
    "        return lowest_dense_res * 2 if 't' not in self.opts else lowest_dense_res\n",
    "\n",
    "    def get_out_ch(self):\n",
    "        return self.ae_out_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_ch is multiplied by 4 -> understand \"image channels\"\n",
    "\n",
    "dec = Decoder(in_ch=ae_dims, d_ch=d_dims, d_mask_ch=d_mask_dims,opts=\"up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand((1,128,32,32))\n",
    "y = dec(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (upscale0): Upscale(\n",
      "    (conv1): Conv2d(128, 2048, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (upscale1): Upscale(\n",
      "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (upscale2): Upscale(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (res0): ResidualBlock(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (res1): ResidualBlock(\n",
      "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (res2): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (upscalem0): Upscale(\n",
      "    (conv1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (upscalem1): Upscale(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (upscalem2): Upscale(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (activation): LeakyReLU(negative_slope=0.1)\n",
      "  )\n",
      "  (out_conv): Conv2d(128, 3, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      "  (out_convm): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
      ")\n",
      "num of params 15415844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "params = sum([np.prod(p.size()) for p in dec.parameters()])\n",
    "print(dec)\n",
    "print(\"num of params\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "padding='same' is not supported for strided convolutions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thoma\\ENSTA\\deep_fake\\deep_face_lab\\pytorch\\PipeLine.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(in_ch\u001b[39m=\u001b[39;49minput_ch, e_ch\u001b[39m=\u001b[39;49me_dims,)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoder_out_ch \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mget_out_ch()\u001b[39m*\u001b[39mencoder\u001b[39m.\u001b[39mget_out_res(resolution)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m inter \u001b[39m=\u001b[39m Inter (in_ch\u001b[39m=\u001b[39mencoder_out_ch, ae_ch\u001b[39m=\u001b[39mae_dims, ae_out_ch\u001b[39m=\u001b[39mae_dims)\n",
      "\u001b[1;32mc:\\Users\\thoma\\ENSTA\\deep_fake\\deep_face_lab\\pytorch\\PipeLine.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mres5 \u001b[39m=\u001b[39m ResidualBlock(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39me_ch\u001b[39m*\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown1 \u001b[39m=\u001b[39m DownscaleBlock(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_ch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49me_ch, n_downscales\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mopts \u001b[39melse\u001b[39;49;00m \u001b[39m5\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\thoma\\ENSTA\\deep_fake\\deep_face_lab\\pytorch\\PipeLine.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_downscales):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     cur_ch \u001b[39m=\u001b[39m ch\u001b[39m*\u001b[39m( \u001b[39mmin\u001b[39m(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mi, \u001b[39m8\u001b[39m)  )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdowns\u001b[39m.\u001b[39mappend ( Downscale(last_ch, cur_ch, kernel_size\u001b[39m=\u001b[39;49mkernel_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     last_ch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdowns[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mget_out_ch()\n",
      "\u001b[1;32mc:\\Users\\thoma\\ENSTA\\deep_fake\\deep_face_lab\\pytorch\\PipeLine.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_ch \u001b[39m=\u001b[39m out_ch\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size \u001b[39m=\u001b[39m kernel_size\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mConv2d(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_ch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_ch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                        stride\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, padding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msame\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/ENSTA/deep_fake/deep_face_lab/pytorch/PipeLine.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLeakyReLU(negative_slope\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\.conda\\envs\\dfl\\lib\\site-packages\\torch\\nn\\modules\\conv.py:447\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    445\u001b[0m padding_ \u001b[39m=\u001b[39m padding \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(padding, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    446\u001b[0m dilation_ \u001b[39m=\u001b[39m _pair(dilation)\n\u001b[1;32m--> 447\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    448\u001b[0m     in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n\u001b[0;32m    449\u001b[0m     \u001b[39mFalse\u001b[39;00m, _pair(\u001b[39m0\u001b[39m), groups, bias, padding_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\.conda\\envs\\dfl\\lib\\site-packages\\torch\\nn\\modules\\conv.py:98\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     96\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid padding string \u001b[39m\u001b[39m{\u001b[39;00mpadding\u001b[39m!r}\u001b[39;00m\u001b[39m, should be one of \u001b[39m\u001b[39m{\u001b[39;00mvalid_padding_strings\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m padding \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(s \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m stride):\n\u001b[1;32m---> 98\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpadding=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not supported for strided convolutions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m valid_padding_modes \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreflect\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mreplicate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcircular\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m padding_mode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_padding_modes:\n",
      "\u001b[1;31mValueError\u001b[0m: padding='same' is not supported for strided convolutions"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(in_ch=input_ch, e_ch=e_dims,)\n",
    "encoder_out_ch = encoder.get_out_ch()*encoder.get_out_res(resolution)**2\n",
    "\n",
    "inter = Inter (in_ch=encoder_out_ch, ae_ch=ae_dims, ae_out_ch=ae_dims)\n",
    "inter_out_ch = inter.get_out_ch()\n",
    "\n",
    "decoder_src = Decoder(in_ch=inter_out_ch, d_ch=d_dims, d_mask_ch=d_mask_dims,opts=\"ud\")\n",
    "decoder_dst = Decoder(in_ch=inter_out_ch, d_ch=d_dims, d_mask_ch=d_mask_dims,opts=\"ud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder()\n",
      "num of params 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "params = sum([np.prod(p.size()) for p in encoder.parameters()])\n",
    "print(encoder)\n",
    "print(\"num of params\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
